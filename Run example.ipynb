{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: torch in /opt/anaconda3/envs/bias-detection/lib/python3.12/site-packages (2.5.1)\n",
      "Requirement already satisfied: filelock in /opt/anaconda3/envs/bias-detection/lib/python3.12/site-packages (from torch) (3.16.1)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in /opt/anaconda3/envs/bias-detection/lib/python3.12/site-packages (from torch) (4.12.2)\n",
      "Requirement already satisfied: networkx in /opt/anaconda3/envs/bias-detection/lib/python3.12/site-packages (from torch) (3.4.2)\n",
      "Requirement already satisfied: jinja2 in /opt/anaconda3/envs/bias-detection/lib/python3.12/site-packages (from torch) (3.1.4)\n",
      "Requirement already satisfied: fsspec in /opt/anaconda3/envs/bias-detection/lib/python3.12/site-packages (from torch) (2024.10.0)\n",
      "Requirement already satisfied: setuptools in /opt/anaconda3/envs/bias-detection/lib/python3.12/site-packages (from torch) (75.1.0)\n",
      "Requirement already satisfied: sympy==1.13.1 in /opt/anaconda3/envs/bias-detection/lib/python3.12/site-packages (from torch) (1.13.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /opt/anaconda3/envs/bias-detection/lib/python3.12/site-packages (from sympy==1.13.1->torch) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /opt/anaconda3/envs/bias-detection/lib/python3.12/site-packages (from jinja2->torch) (3.0.2)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers>=4.45.0 in /opt/anaconda3/envs/bias-detection/lib/python3.12/site-packages (4.46.3)\n",
      "Collecting accelerate\n",
      "  Downloading accelerate-1.12.0-py3-none-any.whl.metadata (19 kB)\n",
      "Collecting bitsandbytes\n",
      "  Downloading bitsandbytes-0.42.0-py3-none-any.whl.metadata (9.9 kB)\n",
      "Requirement already satisfied: huggingface-hub in /opt/anaconda3/envs/bias-detection/lib/python3.12/site-packages (0.26.2)\n",
      "Requirement already satisfied: filelock in /opt/anaconda3/envs/bias-detection/lib/python3.12/site-packages (from transformers>=4.45.0) (3.16.1)\n",
      "Requirement already satisfied: numpy>=1.17 in /opt/anaconda3/envs/bias-detection/lib/python3.12/site-packages (from transformers>=4.45.0) (1.26.4)\n",
      "Requirement already satisfied: packaging>=20.0 in /opt/anaconda3/envs/bias-detection/lib/python3.12/site-packages (from transformers>=4.45.0) (24.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /opt/anaconda3/envs/bias-detection/lib/python3.12/site-packages (from transformers>=4.45.0) (6.0.2)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /opt/anaconda3/envs/bias-detection/lib/python3.12/site-packages (from transformers>=4.45.0) (2024.11.6)\n",
      "Requirement already satisfied: requests in /opt/anaconda3/envs/bias-detection/lib/python3.12/site-packages (from transformers>=4.45.0) (2.32.3)\n",
      "Requirement already satisfied: tokenizers<0.21,>=0.20 in /opt/anaconda3/envs/bias-detection/lib/python3.12/site-packages (from transformers>=4.45.0) (0.20.3)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in /opt/anaconda3/envs/bias-detection/lib/python3.12/site-packages (from transformers>=4.45.0) (0.4.5)\n",
      "Requirement already satisfied: tqdm>=4.27 in /opt/anaconda3/envs/bias-detection/lib/python3.12/site-packages (from transformers>=4.45.0) (4.67.0)\n",
      "Requirement already satisfied: psutil in /opt/anaconda3/envs/bias-detection/lib/python3.12/site-packages (from accelerate) (5.9.0)\n",
      "Requirement already satisfied: torch>=2.0.0 in /opt/anaconda3/envs/bias-detection/lib/python3.12/site-packages (from accelerate) (2.5.1)\n",
      "Requirement already satisfied: scipy in /opt/anaconda3/envs/bias-detection/lib/python3.12/site-packages (from bitsandbytes) (1.14.1)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /opt/anaconda3/envs/bias-detection/lib/python3.12/site-packages (from huggingface-hub) (2024.10.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /opt/anaconda3/envs/bias-detection/lib/python3.12/site-packages (from huggingface-hub) (4.12.2)\n",
      "Requirement already satisfied: networkx in /opt/anaconda3/envs/bias-detection/lib/python3.12/site-packages (from torch>=2.0.0->accelerate) (3.4.2)\n",
      "Requirement already satisfied: jinja2 in /opt/anaconda3/envs/bias-detection/lib/python3.12/site-packages (from torch>=2.0.0->accelerate) (3.1.4)\n",
      "Requirement already satisfied: setuptools in /opt/anaconda3/envs/bias-detection/lib/python3.12/site-packages (from torch>=2.0.0->accelerate) (75.1.0)\n",
      "Requirement already satisfied: sympy==1.13.1 in /opt/anaconda3/envs/bias-detection/lib/python3.12/site-packages (from torch>=2.0.0->accelerate) (1.13.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /opt/anaconda3/envs/bias-detection/lib/python3.12/site-packages (from sympy==1.13.1->torch>=2.0.0->accelerate) (1.3.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/anaconda3/envs/bias-detection/lib/python3.12/site-packages (from requests->transformers>=4.45.0) (3.4.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/anaconda3/envs/bias-detection/lib/python3.12/site-packages (from requests->transformers>=4.45.0) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/anaconda3/envs/bias-detection/lib/python3.12/site-packages (from requests->transformers>=4.45.0) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/anaconda3/envs/bias-detection/lib/python3.12/site-packages (from requests->transformers>=4.45.0) (2024.8.30)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /opt/anaconda3/envs/bias-detection/lib/python3.12/site-packages (from jinja2->torch>=2.0.0->accelerate) (3.0.2)\n",
      "Downloading accelerate-1.12.0-py3-none-any.whl (380 kB)\n",
      "Downloading bitsandbytes-0.42.0-py3-none-any.whl (105.0 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m105.0/105.0 MB\u001b[0m \u001b[31m5.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m0:02\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: bitsandbytes, accelerate\n",
      "Successfully installed accelerate-1.12.0 bitsandbytes-0.42.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install \"transformers>=4.45.0\" accelerate bitsandbytes huggingface-hub"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use the following code to check the login status with huggingface and llama.\n",
    "\n",
    "Make sure:\n",
    "1. The role is read, not fine-grained.\n",
    "```bash\n",
    "'role': 'read'\n",
    "```\n",
    "2. model is ok to process.\n",
    "```bash\n",
    "model ok, id: meta-llama/Llama-3.2-3B-Instruct\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/bias-detection/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "whoami: {'type': 'user', 'id': '691e14c9ad200e6e3b2e2c1d', 'name': 'juan27111', 'fullname': 'jingyuan liu', 'email': 'ljjjuan@bu.edu', 'emailVerified': True, 'canPay': False, 'billingMode': 'prepaid', 'periodEnd': 1767225600, 'isPro': False, 'avatarUrl': '/avatars/c72f708bdb7c43b1fb2d5c2f29a65201.svg', 'orgs': [{'type': 'org', 'id': '63864e015ea0bcbedb132e5a', 'name': 'BostonU', 'fullname': 'Boston University', 'email': None, 'canPay': False, 'billingMode': 'postpaid', 'periodEnd': None, 'avatarUrl': 'https://cdn-avatars.huggingface.co/v1/production/uploads/1669746226855-63864a6a06858a85f592d5ae.png', 'roleInOrg': 'write', 'isEnterprise': False}], 'auth': {'type': 'access_token', 'accessToken': {'displayName': 'localllm', 'role': 'read', 'createdAt': '2025-12-05T01:49:42.167Z'}}}\n",
      "model ok, id: meta-llama/Llama-3.2-3B-Instruct\n"
     ]
    }
   ],
   "source": [
    "from huggingface_hub import whoami, model_info\n",
    "\n",
    "# check whether you have been logged in\n",
    "try:\n",
    "    print(\"whoami:\", whoami())\n",
    "except Exception as e:\n",
    "    print(\"whoami error:\", e)\n",
    "\n",
    "# check whether you have access to the model\n",
    "try:\n",
    "    info = model_info(\"meta-llama/Llama-3.2-3B-Instruct\")\n",
    "    print(\"model ok, id:\", info.id)\n",
    "except Exception as e:\n",
    "    print(\"model_info error:\", e)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the following code to load the model and access the local version. The first try would be a bit long, downloading models.\n",
    "\n",
    "Remember to include your own standard of scoring into the evaluation prompt.\n",
    "\n",
    "Parameters are also adjustable for specific needs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:21<00:00, 10.52s/it]\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "import torch\n",
    "import re\n",
    "\n",
    "MODEL_NAME = \"meta-llama/Llama-3.2-3B-Instruct\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    MODEL_NAME,\n",
    "    torch_dtype=torch.float16,\n",
    "    device_map=\"cpu\"\n",
    ")\n",
    "\n",
    "if model.config.pad_token_id is None and tokenizer.eos_token_id is not None:\n",
    "    model.config.pad_token_id = tokenizer.eos_token_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def score_license(license_text: str) -> float:\n",
    "    system_prompt = (\n",
    "        \"You are a strict security and risk assessment expert. \"\n",
    "        \"Your ONLY job is to read the LICENSE text and output a single numeric score between 0 and 1. \"\n",
    "        \"Do NOT explain, do NOT repeat the prompt, do NOT add any text.\"\n",
    "    )\n",
    "\n",
    "    user_prompt = f\"\"\"\n",
    "Evaluate whether the following LICENSE text is sufficiently safe and detailed.\n",
    "\n",
    "Criteria:\n",
    "1) Safety safeguards against misuse (legal + ethical)\n",
    "2) Detail about permissions, restrictions, and obligations\n",
    "3) Explicit conditions preventing harmful applications\n",
    "\n",
    "Scoring:\n",
    "- 0 = not mentioning license related information at all\n",
    "- 0.5 = not enough details on license\n",
    "- 1 = fully safe and sufficiently detailed\n",
    "- Output a SINGLE number between 0 and 1 (inclusive).\n",
    "- Do NOT output anything except the number.\n",
    "\n",
    "LICENSE:\n",
    "{license_text}\n",
    "\"\"\"\n",
    "\n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": system_prompt},\n",
    "        {\"role\": \"user\", \"content\": user_prompt},\n",
    "    ]\n",
    "    prompt = tokenizer.apply_chat_template(\n",
    "        messages,\n",
    "        tokenize=False,\n",
    "        add_generation_prompt=True,  \n",
    "    )\n",
    "\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n",
    "\n",
    "    outputs = model.generate(\n",
    "        **inputs,\n",
    "        max_new_tokens=8,     \n",
    "        do_sample=False,\n",
    "        pad_token_id=tokenizer.eos_token_id,\n",
    "    )\n",
    "\n",
    "    generated_ids = outputs[0][inputs[\"input_ids\"].shape[-1]:]\n",
    "    answer = tokenizer.decode(generated_ids, skip_special_tokens=True).strip()\n",
    "\n",
    "    score = float(answer)\n",
    "    print(score)\n",
    "    return score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/bias-detection/lib/python3.12/site-packages/transformers/generation/configuration_utils.py:590: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/envs/bias-detection/lib/python3.12/site-packages/transformers/generation/configuration_utils.py:595: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.25\n"
     ]
    }
   ],
   "source": [
    "example_text = 'Our model and weights are licensed for both researchers and commercial entities, upholding the principles of openness. Our mission is to empower individuals and industry, through this opportunity, while fostering an environment of discovery and ethical AI advancements.'\n",
    "example_score = score_license(example_text)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
